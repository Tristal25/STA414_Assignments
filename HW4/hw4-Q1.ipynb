{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating the data\n",
    "\n",
    "First, we will generate some data for this problem. Set the number of points $N=400$, their dimension $D=2$, and the number of clusters $K=2$, and generate data from the distribution $p(x|z=k) = \\mathcal{N}(\\mu_k, \\Sigma_k)$.\n",
    "  Sample $200$ data points for $k=1$ and 200 for $k=2$, with\n",
    "\n",
    "  $$\n",
    "    \\mu_1=\n",
    "    \\begin{bmatrix}\n",
    "      0.1 \\\\\n",
    "      0.1\n",
    "    \\end{bmatrix}\n",
    "    \\ \\text{,}\\\n",
    "    \\mu_2=\n",
    "    \\begin{bmatrix}\n",
    "      6.0 \\\\\n",
    "      0.1\n",
    "    \\end{bmatrix}\n",
    "    \\ \\text{ and }\\\n",
    "    \\Sigma_1=\\Sigma_2=\n",
    "    \\begin{bmatrix}\n",
    "      10       & 7 \\\\\n",
    "      7 & 10\n",
    "    \\end{bmatrix}\n",
    "  $$\n",
    "  Here, $N=400$. Since you generated the data, you already know which sample comes from which class.\n",
    "  Run the cell in the IPython notebook to generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this cell to generate the data\n",
    "num_samples = 400\n",
    "cov = np.array([[1., .7], [.7, 1.]]) * 10\n",
    "mean_1 = [.1, .1]\n",
    "mean_2 = [6., .1]\n",
    "\n",
    "x_class1 = np.random.multivariate_normal(mean_1, cov, num_samples // 2)\n",
    "x_class2 = np.random.multivariate_normal(mean_2, cov, num_samples // 2)\n",
    "xy_class1 = np.column_stack((x_class1, np.zeros(num_samples // 2)))\n",
    "xy_class2 = np.column_stack((x_class2, np.ones(num_samples // 2)))\n",
    "data_full = np.row_stack([xy_class1, xy_class2])\n",
    "np.random.shuffle(data_full)\n",
    "data = data_full[:, :2]\n",
    "labels = data_full[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a scatter plot of the data points showing the true cluster assignment of each point using different color codes and shape (x for first class and circles for second class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make a scatterplot for the data points showing the true cluster assignments of each point\n",
    "# plt.plot(...) # first class, x shape\n",
    "# plt.plot(...) # second class, circle shape\n",
    "c0 = data[labels==0, :]\n",
    "c1 = data[labels==1, :]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(c0[:, 0], c0[:, 1], 'x')\n",
    "plt.plot(c1[:, 0], c1[:, 1], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement and Run K-Means algorithm\n",
    "\n",
    "Now, we assume that the true class labels are not known. Implement the k-means algorithm for this problem.\n",
    "  Write two functions: `km_assignment_step`, and `km_refitting_step` as given in the lecture (Here, `km_` means k-means).\n",
    "  Identify the correct arguments, and the order to run them. Initialize the algorithm with\n",
    "  $$\n",
    "    \\hat\\mu_1=\n",
    "    \\begin{bmatrix}\n",
    "      0.0 \\\\\n",
    "      0.0\n",
    "    \\end{bmatrix}\n",
    "    \\ \\text{,}\\\n",
    "    \\hat\\mu_2=\n",
    "    \\begin{bmatrix}\n",
    "      1.0 \\\\\n",
    "      1.0\n",
    "    \\end{bmatrix}\n",
    "  $$\n",
    "  and run it until convergence.\n",
    "  Show the resulting cluster assignments on a scatter plot either using different color codes or shape or both.\n",
    "  Also plot the cost vs. the number of iterations. Report your misclassification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(data, R, Mu):\n",
    "    N, D = data.shape\n",
    "    K = Mu.shape[1]\n",
    "    J = 0\n",
    "    for k in range(K):\n",
    "        J += np.dot(np.linalg.norm(data - np.array([Mu[:, k], ] * N), axis=1)**2, R[:, k])\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: K-Means Assignment Step\n",
    "def km_assignment_step(data, Mu):\n",
    "    \"\"\" Compute K-Means assignment step\n",
    "    \n",
    "    Args:\n",
    "        data: a NxD matrix for the data points\n",
    "        Mu: a DxK matrix for the cluster means locations\n",
    "    \n",
    "    Returns:\n",
    "        R_new: a NxK matrix of responsibilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill this in:\n",
    "    N, D = data.shape\n",
    "    K = Mu.shape[1]\n",
    "    r = np.zeros((N, K))\n",
    "    for k in range(K):\n",
    "        err = data - np.expand_dims(Mu[:, k], axis=0)\n",
    "        r[:, k] = np.sum(err**2, axis=1)\n",
    "    arg_min = np.argmin(r, axis=1)\n",
    "    R_new = np.zeros((N, K)) \n",
    "    R_new[list(range(N)), arg_min] = 1\n",
    "    return R_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: K-means Refitting Step\n",
    "def km_refitting_step(data, R, Mu):\n",
    "    \"\"\" Compute K-Means refitting step.\n",
    "    \n",
    "    Args:\n",
    "        data: a NxD matrix for the data points\n",
    "        R: a NxK matrix of responsibilities\n",
    "        Mu: a DxK matrix for the cluster means locations\n",
    "    \n",
    "    Returns:\n",
    "        Mu_new: a DxK matrix for the new cluster means locations\n",
    "    \"\"\"\n",
    "    N, D = data.shape \n",
    "    K = Mu.shape[1]\n",
    "    Mu_new = (data.T @ R) / np.sum(R, axis=0)\n",
    "    return Mu_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this cell to call the K-means algorithm\n",
    "N, D = data.shape\n",
    "K = 2\n",
    "max_iter = 100\n",
    "class_init = np.random.binomial(1., .5, size=N)\n",
    "R = np.vstack([class_init, 1 - class_init]).T\n",
    "\n",
    "Mu = np.zeros([D, K])\n",
    "Mu[:, 1] = 1.\n",
    "R.T.dot(data), np.sum(R, axis=0)\n",
    "\n",
    "c = []\n",
    "for it in range(max_iter):\n",
    "    R = km_assignment_step(data, Mu)\n",
    "    Mu = km_refitting_step(data, R, Mu)\n",
    "    c.append(cost(data, R, Mu))\n",
    "    #print(it, cost(data, R, Mu))\n",
    "    \n",
    "class_1 = np.where(R[:, 0])\n",
    "class_2 = np.where(R[:, 1])\n",
    "\n",
    "miss_err = np.sum(R[:, 0]==labels)/N\n",
    "print('The missclassification error is: %f'  % miss_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost vs. the number of iterations\n",
    "plt.figure()\n",
    "plt.title('Cost vs. # of iterations')\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations')\n",
    "plt.plot(np.arange(max_iter), c, 'm-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make a scatterplot for the data points showing the K-Means cluster assignments of each point\n",
    "d0 = data[class_1]\n",
    "d1 = data[class_2]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(d0[:, 0], d0[:, 1], 'x')\n",
    "plt.plot(d1[:, 0], d1[:, 1], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement EM algorithm for Gaussian mixtures\n",
    "Next, implement the EM algorithm for Gaussian mixtures.\n",
    "  Write three functions: `log_likelihood`, `gm_e_step`, and `gm_m_step` as given in the lecture.\n",
    "  Identify the correct arguments, and the order to run them.\n",
    "  Initialize the algorithm with the same initialization as in Q2.1 for the means, and with $\\hat\\Sigma_1=\\hat\\Sigma_2=I$,\n",
    "  and $\\hat\\pi_1=\\hat\\pi_2$ for the covariances.\n",
    "    \n",
    "  Run the algorithm until convergence and show the resulting cluster assignments on a scatter plot either using different color codes or shape or both.\n",
    "  Also plot the log-likelihood vs. the number of iterations. Report your misclassification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_density(x, mu, Sigma):\n",
    "    return np.exp(-.5 * np.dot(x - mu, np.linalg.solve(Sigma, x - mu))) \\\n",
    "        / np.sqrt(np.linalg.det(2 * np.pi * Sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(data, Mu, Sigma, Pi):\n",
    "    \"\"\" Compute log likelihood on the data given the Gaussian Mixture Parameters.\n",
    "    \n",
    "    Args:\n",
    "        data: a NxD matrix for the data points\n",
    "        Mu: a DxK matrix for the means of the K Gaussian Mixtures\n",
    "        Sigma: a list of size K with each element being DxD covariance matrix\n",
    "        Pi: a vector of size K for the mixing coefficients\n",
    "    \n",
    "    Returns:\n",
    "        L: a scalar denoting the log likelihood of the data given the Gaussian Mixture\n",
    "    \"\"\"\n",
    "    # Fill this in:\n",
    "    N, D = data.shape\n",
    "    K = Mu.shape[1]\n",
    "    L, T = 0., 0.\n",
    "    for n in range(N):\n",
    "        T = 0\n",
    "        for k in range(K):\n",
    "            T += Pi[k] * normal_density(data[n], Mu[:, k], Sigma[k])\n",
    "        L += np.log(T)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Gaussian Mixture Expectation Step\n",
    "def gm_e_step(data, Mu, Sigma, Pi):\n",
    "    \"\"\" Gaussian Mixture Expectation Step.\n",
    "\n",
    "    Args:\n",
    "        data: a NxD matrix for the data points\n",
    "        Mu: a DxK matrix for the means of the K Gaussian Mixtures\n",
    "        Sigma: a list of size K with each element being DxD covariance matrix\n",
    "        Pi: a vector of size K for the mixing coefficients\n",
    "    \n",
    "    Returns:\n",
    "        Gamma: a NxK matrix of responsibilities \n",
    "    \"\"\"\n",
    "    # Fill this in:\n",
    "    N, D = data.shape\n",
    "    K = Mu.shape[1]\n",
    "    Gamma = np.zeros((N,K))\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            Gamma[n, k] = Pi[k] * normal_density(data[n], Mu[:, k], Sigma[k]) \n",
    "        Gamma[n, :] /= np.sum(Gamma[n, :]) \n",
    "    return Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Gaussian Mixture Maximization Step\n",
    "def gm_m_step(data, Gamma):\n",
    "    \"\"\" Gaussian Mixture Maximization Step.\n",
    "\n",
    "    Args:\n",
    "        data: a NxD matrix for the data points\n",
    "        Gamma: a NxK matrix of responsibilities \n",
    "    \n",
    "    Returns:\n",
    "        Mu: a DxK matrix for the means of the K Gaussian Mixtures\n",
    "        Sigma: a list of size K with each element being DxD covariance matrix\n",
    "        Pi: a vector of size K for the mixing coefficients\n",
    "    \"\"\"\n",
    "    # Fill this in:\n",
    "    N, D = data.shape \n",
    "    K = Gamma.shape[1] \n",
    "    Nk = np.sum(Gamma, axis=0) \n",
    "    Mu = np.zeros([D, K]) \n",
    "    Sigma = np.zeros([K, D, D])\n",
    "\n",
    "    for k in range(K):\n",
    "        Mu[k] = (data.T @ Gamma)[k] / Nk[k]\n",
    "        sigmasum = np.zeros([D,D])\n",
    "        for n in range(N):\n",
    "            xmmu = data[n:n+1].T - Mu[:,k:k+1]\n",
    "            sigmasum += Gamma[n,k] * (xmmu @ xmmu.T)\n",
    "        Sigma[k] = sigmasum / Nk[k]\n",
    "    Pi = Nk / N \n",
    "    return Mu, Sigma, Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this cell to call the Gaussian Mixture EM algorithm\n",
    "N, D = data.shape\n",
    "K = 2\n",
    "Mu = np.zeros([D, K])\n",
    "Mu[:, 1] = 1.\n",
    "Sigma = [np.eye(2), np.eye(2)]\n",
    "Pi = np.ones(K) / K\n",
    "Gamma = np.zeros([N, K]) # Gamma is the matrix of responsibilities \n",
    "\n",
    "max_iter  = 200\n",
    "\n",
    "loglik = []\n",
    "for it in range(max_iter):\n",
    "    Gamma = gm_e_step(data, Mu, Sigma, Pi)\n",
    "    Mu, Sigma, Pi = gm_m_step(data, Gamma)\n",
    "    loglik.append(log_likelihood(data, Mu, Sigma, Pi))\n",
    "    # print(it, log_likelihood(data, Mu, Sigma, Pi)) # This function makes the computation longer, but good for debugging\n",
    "\n",
    "class_1 = np.where(Gamma[:, 0] >= .5)\n",
    "class_2 = np.where(Gamma[:, 1] >= .5)\n",
    "\n",
    "miss_err = np.sum(np.float32(Gamma[:, 0] >= .5)==labels)/N\n",
    "print('The missclassification error is: %f'  % miss_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost vs. the number of iterations\n",
    "plt.figure()\n",
    "plt.title('log-likelihood vs. # of iterations')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.xlabel('iterations')\n",
    "plt.plot(np.arange(max_iter), loglik, 'm-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make a scatterplot for the data points showing the Gaussian Mixture cluster assignments of each point\n",
    "d0 = data[class_1]\n",
    "d1 = data[class_2]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(d0[:, 0], d0[:, 1], 'x')\n",
    "plt.plot(d1[:, 0], d1[:, 1], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comment on findings + additional experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the results:\n",
    "\n",
    "* Compare the performance of k-Means and EM based on the resulting cluster assignments.\n",
    "* Compare the performance of k-Means and EM based on their convergence rate. What is the bottleneck for which method?\n",
    "* Experiment with 5 different data realizations (generate new data), run your algorithms, and summarize your findings. Does the algorithm performance depend on different realizations of data?\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Your written answer here**\n",
    "\n",
    "- EM algorithm gives a much more accurate classification. The missclassification error is lower and the scatterplot is more similar to the true one. \n",
    "\n",
    "- K-means converge faster than EM algorithm. The bottleneck of the two algorithms are about 5 for k-means and about 25 for EM algorithm. \n",
    "\n",
    "- The misclassification error of k-means are always higher than that of EM algorithm. So the performance does not depend on different realization of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
