{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data, load, save, display_plot\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_nn(num_inputs, num_hiddens, num_outputs):\n",
    "    \"\"\" Initializes neural network's parameters.\n",
    "    :param num_inputs: Number of input units\n",
    "    :param num_hiddens: List of two elements, hidden size for each layers.\n",
    "    :param num_outputs: Number of output units\n",
    "    :return: A dictionary of randomly initialized neural network weights.\n",
    "    \"\"\"\n",
    "    W1 = 0.1 * np.random.randn(num_inputs, num_hiddens[0])\n",
    "    W2 = 0.1 * np.random.randn(num_hiddens[0], num_hiddens[1])\n",
    "    W3 = 0.01 * np.random.randn(num_hiddens[1], num_outputs)\n",
    "    b1 = np.zeros((num_hiddens[0]))\n",
    "    b2 = np.zeros((num_hiddens[1]))\n",
    "    b3 = np.zeros((num_outputs))\n",
    "    model = {\n",
    "        \"W1\": W1,\n",
    "        \"W2\": W2,\n",
    "        \"W3\": W3,\n",
    "        \"b1\": b1,\n",
    "        \"b2\": b2,\n",
    "        \"b3\": b3\n",
    "    }\n",
    "    return model\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\" Computes the softmax activation function.\n",
    "    :param x: Inputs\n",
    "    :return: Activation of x\n",
    "    \"\"\"\n",
    "    return np.exp(x) / np.exp(x).sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def nn_forward(model, x):\n",
    "    \"\"\" Runs a forward pass.\n",
    "    :param model: Dictionary of all the weights.\n",
    "    :param x: Input to the network.\n",
    "    :return: Dictionary of all intermediate variables.\n",
    "    \"\"\"\n",
    "    z1 = affine(x, model[\"W1\"], model[\"b1\"])\n",
    "    h1 = relu(z1)\n",
    "    z2 = affine(h1, model[\"W2\"], model[\"b2\"])\n",
    "    h2 = relu(z2)\n",
    "    y = affine(h2, model[\"W3\"], model[\"b3\"])\n",
    "    var = {\n",
    "        \"x\": x,\n",
    "        \"z1\": z1,\n",
    "        \"h1\": h1,\n",
    "        \"z2\": z2,\n",
    "        \"h2\": h2,\n",
    "        \"y\": y\n",
    "    }\n",
    "    return var\n",
    "\n",
    "def nn_backward(model, err, var):\n",
    "    \"\"\" Runs the backward pass.\n",
    "    :param model: Dictionary of all the weights.\n",
    "    :param err: Gradients to the output of the network.\n",
    "    :param var: Intermediate variables from the forward pass.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    dE_dh2, dE_dW3, dE_db3 = affine_backward(err, var[\"h2\"], model[\"W3\"])\n",
    "    dE_dz2 = relu_backward(dE_dh2, var[\"z2\"])\n",
    "    dE_dh1, dE_dW2, dE_db2 = affine_backward(dE_dz2, var[\"h1\"], model[\"W2\"])\n",
    "    dE_dz1 = relu_backward(dE_dh1, var[\"z1\"])\n",
    "    _, dE_dW1, dE_db1 = affine_backward(dE_dz1, var[\"x\"], model[\"W1\"])\n",
    "    model[\"dE_dW1\"] = dE_dW1\n",
    "    model[\"dE_dW2\"] = dE_dW2\n",
    "    model[\"dE_dW3\"] = dE_dW3\n",
    "    model[\"dE_db1\"] = dE_db1\n",
    "    model[\"dE_db2\"] = dE_db2\n",
    "    model[\"dE_db3\"] = dE_db3\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine(x, w, b):\n",
    "    \"\"\" Computes the affine transformation.\n",
    "    :param x: Inputs (or hidden layers)\n",
    "    :param w: Weight\n",
    "    :param b: Bias\n",
    "    :return: Outputs\n",
    "    \"\"\"\n",
    "    y = x.dot(w) + b\n",
    "    return y\n",
    "\n",
    "\n",
    "def affine_backward(grad_y, x, w):\n",
    "    \"\"\" Computes gradients of affine transformation.\n",
    "    Hint: you may need the matrix transpose np.dot(A, B).T = np.dot(B, A) and (A.T).T = A\n",
    "    :param grad_y: Gradient from upper layer\n",
    "    :param x: Inputs from the hidden layer\n",
    "    :param w: Weights\n",
    "    :return: A tuple of (grad_h, grad_w, grad_b)\n",
    "        WHERE\n",
    "        grad_x: Gradients wrt. the inputs/hidden layer.\n",
    "        grad_w: Gradients wrt. the weights.\n",
    "        grad_b: Gradients wrt. the biases.\n",
    "    \"\"\"\n",
    "    #####################################################################\n",
    "    # TODO:                                                             #\n",
    "    # Complete the function to compute the gradients of affine          #\n",
    "    # transformation.                                                   #\n",
    "    #####################################################################\n",
    "    grad_x = grad_y @ w.T\n",
    "    grad_w = x.T @ grad_y\n",
    "    grad_b = np.sum(grad_y, axis = 0)\n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    #####################################################################\n",
    "    return grad_x, grad_w, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\" Computes the ReLU activation function.\n",
    "    :param z: Inputs\n",
    "    :return: Activation of x\n",
    "    \"\"\"\n",
    "    return np.maximum(x, 0.0)\n",
    "\n",
    "def relu_backward(grad_y, x):\n",
    "    \"\"\" Computes gradients of the ReLU activation function wrt. the unactivated inputs.\n",
    "    :param grad_y: Gradient of the activation.\n",
    "    :param x: Inputs\n",
    "    :return: Gradient wrt. x\n",
    "    \"\"\"\n",
    "    #####################################################################\n",
    "    # TODO:                                                             #\n",
    "    # Complete the function to compute the gradients of relu.           #\n",
    "    #####################################################################\n",
    "    ydx = np.float32(relu(x)!=0)\n",
    "    grad_x = grad_y * ydx\n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    #####################################################################\n",
    "    return grad_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_update(model, eta):\n",
    "    \"\"\" Update NN weights.\n",
    "    :param model: Dictionary of all the weights.\n",
    "    :param eta: Learning rate\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    #####################################################################\n",
    "    # TODO:                                                             #\n",
    "    # Complete the function to update the neural network's parameters.  #\n",
    "    # Your code should look as follows                                  #\n",
    "    # model[\"W1\"] = ...                                                 #\n",
    "    # model[\"W2\"] = ...                                                 #\n",
    "    # ...                                                               #\n",
    "    #####################################################################\n",
    "    model[\"W1\"] = model[\"W1\"] - eta * model[\"dE_dW1\"]\n",
    "    model[\"W2\"] = model[\"W2\"] - eta * model[\"dE_dW2\"]\n",
    "    model[\"W3\"] = model[\"W3\"] - eta * model[\"dE_dW3\"]\n",
    "    model[\"b1\"] = model[\"b1\"] - eta * model[\"dE_db1\"]\n",
    "    model[\"b2\"] = model[\"b2\"] - eta * model[\"dE_db2\"]\n",
    "    model[\"b3\"] = model[\"b3\"] - eta * model[\"dE_db3\"]\n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    #####################################################################\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, forward, backward, update, eta, num_epochs, batch_size):\n",
    "    \"\"\" Trains a simple MLP.\n",
    "    :param model: Dictionary of model weights.\n",
    "    :param forward: Forward prop function.\n",
    "    :param backward: Backward prop function.\n",
    "    :param update: Update weights function.\n",
    "    :param eta: Learning rate.\n",
    "    :param num_epochs: Number of epochs to run training for.\n",
    "    :param batch_size: Mini-batch size, -1 for full batch.\n",
    "    :return: A tuple (train_ce, valid_ce, train_acc, valid_acc)\n",
    "        WHERE\n",
    "        train_ce: Training cross entropy.\n",
    "        valid_ce: Validation cross entropy.\n",
    "        train_acc: Training accuracy.\n",
    "        valid_acc: Validation accuracy.\n",
    "    \"\"\"\n",
    "    inputs_train, inputs_valid, inputs_test, target_train, target_valid, \\\n",
    "        target_test = load_data(\"toronto_face.npz\")\n",
    "    rnd_idx = np.arange(inputs_train.shape[0])\n",
    "    \n",
    "    train_ce_list = []\n",
    "    valid_ce_list = []\n",
    "    train_acc_list = []\n",
    "    valid_acc_list = []\n",
    "    num_train_cases = inputs_train.shape[0]\n",
    "    if batch_size == -1:\n",
    "        batch_size = num_train_cases\n",
    "    num_steps = int(np.ceil(num_train_cases / batch_size))\n",
    "    for epoch in range(num_epochs):\n",
    "        np.random.shuffle(rnd_idx)\n",
    "        inputs_train = inputs_train[rnd_idx]\n",
    "        target_train = target_train[rnd_idx]\n",
    "        for step in range(num_steps):\n",
    "            # Forward pass.\n",
    "            start = step * batch_size\n",
    "            end = min(num_train_cases, (step + 1) * batch_size)\n",
    "            x = inputs_train[start: end]\n",
    "            t = target_train[start: end]\n",
    "\n",
    "            var = forward(model, x)\n",
    "            prediction = softmax(var[\"y\"])\n",
    "\n",
    "            train_ce = -np.sum(t * np.log(prediction)) / float(x.shape[0])\n",
    "            train_acc = (np.argmax(prediction, axis=1) ==\n",
    "                         np.argmax(t, axis=1)).astype(\"float\").mean()\n",
    "            print((\"Epoch {:3d} Step {:2d} Train CE {:.5f} \"\n",
    "                   \"Train Acc {:.5f}\").format(\n",
    "                epoch, step, train_ce, train_acc))\n",
    "\n",
    "            # Compute error.\n",
    "            error = (prediction - t) / float(x.shape[0])\n",
    "\n",
    "            # Backward prop.\n",
    "            backward(model, error, var)\n",
    "\n",
    "            # Update weights.\n",
    "            update(model, eta)\n",
    "\n",
    "        valid_ce, valid_acc = evaluate(\n",
    "            inputs_valid, target_valid, model, forward, batch_size=batch_size)\n",
    "        print((\"Epoch {:3d} \"\n",
    "               \"Validation CE {:.5f} \"\n",
    "               \"Validation Acc {:.5f}\\n\").format(\n",
    "            epoch, valid_ce, valid_acc))\n",
    "        train_ce_list.append((epoch, train_ce))\n",
    "        train_acc_list.append((epoch, train_acc))\n",
    "        valid_ce_list.append((epoch, valid_ce))\n",
    "        valid_acc_list.append((epoch, valid_acc))\n",
    "    display_plot(train_ce_list, valid_ce_list, \"Cross Entropy\", number=0)\n",
    "    display_plot(train_acc_list, valid_acc_list, \"Accuracy\", number=1)\n",
    "\n",
    "    train_ce, train_acc = evaluate(\n",
    "        inputs_train, target_train, model, forward, batch_size=batch_size)\n",
    "    valid_ce, valid_acc = evaluate(\n",
    "        inputs_valid, target_valid, model, forward, batch_size=batch_size)\n",
    "    test_ce, test_acc = evaluate(\n",
    "        inputs_test, target_test, model, forward, batch_size=batch_size)\n",
    "    print(\"CE: Train %.5f Validation %.5f Test %.5f\" %\n",
    "          (train_ce, valid_ce, test_ce))\n",
    "    print(\"Acc: Train {:.5f} Validation {:.5f} Test {:.5f}\".format(\n",
    "        train_acc, valid_acc, test_acc))\n",
    "\n",
    "    stats = {\n",
    "        \"train_ce\": train_ce_list,\n",
    "        \"valid_ce\": valid_ce_list,\n",
    "        \"train_acc\": train_acc_list,\n",
    "        \"valid_acc\": valid_acc_list\n",
    "    }\n",
    "\n",
    "    return model, stats\n",
    "\n",
    "\n",
    "def evaluate(inputs, target, model, forward, batch_size=-1):\n",
    "    \"\"\" Evaluates the model on inputs and target.\n",
    "    :param inputs: Inputs to the network\n",
    "    :param target: Target of the inputs\n",
    "    :param model: Dictionary of network weights\n",
    "    :param forward: Function for forward pass\n",
    "    :param batch_size: Batch size\n",
    "    :return: A tuple (ce, acc)\n",
    "        WHERE\n",
    "        ce: cross entropy\n",
    "        acc: accuracy\n",
    "    \"\"\"\n",
    "    num_cases = inputs.shape[0]\n",
    "    if batch_size == -1:\n",
    "        batch_size = num_cases\n",
    "    num_steps = int(np.ceil(num_cases / batch_size))\n",
    "    ce = 0.0\n",
    "    acc = 0.0\n",
    "    for step in range(num_steps):\n",
    "        start = step * batch_size\n",
    "        end = min(num_cases, (step + 1) * batch_size)\n",
    "        x = inputs[start: end]\n",
    "        t = target[start: end]\n",
    "        prediction = softmax(forward(model, x)[\"y\"])\n",
    "        ce += -np.sum(t * np.log(prediction))\n",
    "        acc += (np.argmax(prediction, axis=1) == np.argmax(\n",
    "            t, axis=1)).astype(\"float\").sum()\n",
    "    ce /= num_cases\n",
    "    acc /= num_cases\n",
    "    return ce, acc\n",
    "\n",
    "\n",
    "def check_grad(model, forward, backward, name, x):\n",
    "    \"\"\" Check the gradients.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    var = forward(model, x)\n",
    "    loss = lambda y: 0.5 * (y ** 2).sum()\n",
    "    grad_y = var[\"y\"]\n",
    "    backward(model, grad_y, var)\n",
    "    grad_w = model[\"dE_d\" + name].ravel()\n",
    "    w_ = model[name].ravel()\n",
    "    eps = 1e-7\n",
    "    grad_w_2 = np.zeros(w_.shape)\n",
    "    check_elem = np.arange(w_.size)\n",
    "    np.random.shuffle(check_elem)\n",
    "    # Randomly check 20 elements.\n",
    "    check_elem = check_elem[:20]\n",
    "    for ii in check_elem:\n",
    "        w_[ii] += eps\n",
    "        err_plus = loss(forward(model, x)[\"y\"])\n",
    "        w_[ii] -= 2 * eps\n",
    "        err_minus = loss(forward(model, x)[\"y\"])\n",
    "        w_[ii] += eps\n",
    "        grad_w_2[ii] = (err_plus - err_minus) / 2. / eps\n",
    "    np.testing.assert_almost_equal(grad_w[check_elem], grad_w_2[check_elem],\n",
    "                                   decimal=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\" Trains a neural network.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    model_file_name = \"nn_model.npz\"\n",
    "    stats_file_name = \"nn_stats.npz\"\n",
    "\n",
    "    # Hyper-parameters. Modify them if needed.\n",
    "    num_hiddens = [16,80]\n",
    "    eta = 0.01\n",
    "    num_epochs = 1000 # Number of iterations\n",
    "    batch_size = 100\n",
    "\n",
    "    # Input-output dimensions.\n",
    "    num_inputs = 2304\n",
    "    num_outputs = 7\n",
    "\n",
    "    # Initialize model.\n",
    "    model = init_nn(num_inputs, num_hiddens, num_outputs)\n",
    "\n",
    "    # Uncomment to reload trained model here.\n",
    "    # model = load(model_file_name)\n",
    "\n",
    "    # Check gradient implementation.\n",
    "    print(\"Checking gradients...\")\n",
    "    x = np.random.rand(10, 48 * 48) * 0.1\n",
    "    check_grad(model, nn_forward, nn_backward, \"W3\", x)\n",
    "    check_grad(model, nn_forward, nn_backward, \"b3\", x)\n",
    "    check_grad(model, nn_forward, nn_backward, \"W2\", x)\n",
    "    check_grad(model, nn_forward, nn_backward, \"b2\", x)\n",
    "    check_grad(model, nn_forward, nn_backward, \"W1\", x)\n",
    "    check_grad(model, nn_forward, nn_backward, \"b1\", x)\n",
    "\n",
    "    # Train model.\n",
    "    model, stats = train(model, nn_forward, nn_backward, nn_update, eta,\n",
    "                  num_epochs, batch_size)\n",
    "\n",
    "    # Uncomment if you wish to save the model.\n",
    "    save(model_file_name, model)\n",
    "\n",
    "    # Uncomment if you wish to save the training statistics.\n",
    "    save(stats_file_name, stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"nn_model.npz\"\n",
    "inputs_train, inputs_valid, inputs_test, target_train, target_valid, \\\n",
    "    target_test = load_data(\"toronto_face.npz\")\n",
    "model = load(model_file_name)\n",
    "var = nn_forward(model, inputs_test)\n",
    "prediction = softmax(var[\"y\"])\n",
    "\n",
    "pred_conf = np.max(prediction, axis = 1)\n",
    "uncertain = pred_conf <= sorted(pred_conf)[4]\n",
    "uncertain_img = inputs_test[uncertain]\n",
    "uncertain_class = target_test[uncertain]\n",
    "uncertain_pred = np.argmax(prediction[uncertain], axis = 1)+1\n",
    "\n",
    "for i in range(5):\n",
    "    plt.figure()\n",
    "    plt.imshow(np.reshape(uncertain_img[i],[48,48]), cmap = \"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    true_class = np.argmax(prediction[uncertain], axis = 1)[i]+1\n",
    "    pred_class = uncertain_pred[i]\n",
    "    plt.title(f\"True Class: {true_class}, Predicted Class: {pred_class}\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
