{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reinforcement Learning\n",
    "\n",
    "There are 3 files:\n",
    "1. `maze.py`: defines the `MazeEnv` class, the simulation environment which the Q-learning agent will interact in.\n",
    "2. `qlearning.py`: defines the `qlearn` function which you will implement, along with several helper functions. Follow the instructions in the file. \n",
    "3. `plotting_utils.py`: defines several plotting and visualization utilities. In particular, you will use `plot_steps_vs_iters`, `plot_several_steps_vs_iters`, `plot_policy_from_q`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from qlearning import qlearn\n",
    "#from maze import MazeEnv, ProbabilisticMazeEnv\n",
    "#from plotting_utils import plot_steps_vs_iters, plot_several_steps_vs_iters, plot_policy_from_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qlearning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "def qlearn(env, num_iters, alpha, gamma, epsilon, max_steps, use_softmax_policy, init_beta=None, k_exp_sched=None):\n",
    "    \"\"\" Runs tabular Q learning algorithm for stochastic environment.\n",
    "\n",
    "    Args:\n",
    "        env: instance of environment object \n",
    "        num_iters (int): Number of episodes to run Q-learning algorithm\n",
    "        alpha (float): The learning rate between [0,1]\n",
    "        gamma (float): Discount factor, between [0,1)\n",
    "        epsilon (float): Probability in [0,1] that the agent selects a random move instead of \n",
    "                selecting greedily from Q value\n",
    "        max_steps (int): Maximum number of steps in the environment per episode\n",
    "        use_softmax_policy (bool): Whether to use softmax policy (True) or Epsilon-Greedy (False)\n",
    "        init_beta (float): If using stochastic policy, sets the initial beta as the parameter for the softmax\n",
    "        k_exp_sched (float): If using stochastic policy, sets hyperparameter for exponential schedule\n",
    "            on beta\n",
    "    \n",
    "    Returns:\n",
    "        q_hat: A Q-value table shaped [num_states, num_actions] for environment with with num_states \n",
    "            number of states (e.g. num rows * num columns for grid) and num_actions number of possible \n",
    "            actions (e.g. 4 actions up/down/left/right)\n",
    "        steps_vs_iters: An array of size num_iters. Each element denotes the number \n",
    "            of steps in the environment that the agent took to get to the goal\n",
    "            (capped to max_steps)\n",
    "    \"\"\"\n",
    "    action_space_size = env.num_actions\n",
    "    state_space_size = env.num_states\n",
    "    q_hat = np.zeros(shape=(state_space_size, action_space_size))\n",
    "    steps_vs_iters = np.zeros(num_iters)\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # TODO: Initialize current state by resetting the environment\n",
    "        curr_state = env.reset()\n",
    "        num_steps = 0\n",
    "        done = False\n",
    "\n",
    "        # TODO: Keep looping while environment isn't done and less than maximum steps\n",
    "        while (num_steps <= max_steps) and (not done):\n",
    "            num_steps += 1\n",
    "\n",
    "            # Choose an action using policy derived from either softmax Q-value \n",
    "            # or epsilon greedy\n",
    "            if use_softmax_policy:\n",
    "                assert(init_beta is not None)\n",
    "                assert(k_exp_sched is not None)\n",
    "                # TODO: Boltzmann stochastic policy (softmax policy)\n",
    "                beta = beta_exp_schedule(init_beta, i, k = k_exp_sched)\n",
    "                action = softmax_policy(q_hat, beta, curr_state)\n",
    "            else:\n",
    "                # TODO: Epsilon-greedy\n",
    "                action = epsilon_greedy(q_hat, epsilon, curr_state, action_space_size)\n",
    "\n",
    "            # TODO: Execute action in the environment and observe the next state, reward, and done flag\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # TODO: Update Q_value\n",
    "            if next_state != curr_state:\n",
    "                new_value = reward + gamma * np.max(q_hat[next_state, :]) - q_hat[curr_state, action]\n",
    "                # TODO: Use Q-learning rule to update q_hat for the curr_state and action:\n",
    "                # i.e., Q(s,a) <- Q(s,a) + alpha*[reward + gamma * max_a'(Q(s',a')) - Q(s,a)]\n",
    "                q_hat[curr_state, action] = q_hat[curr_state, action] + alpha * new_value\n",
    "                # TODO: Update the current staet to be the next state\n",
    "                curr_state = next_state\n",
    "\n",
    "        steps_vs_iters[i] = num_steps\n",
    "    \n",
    "    return q_hat, steps_vs_iters\n",
    "\n",
    "\n",
    "def epsilon_greedy(q_hat, epsilon, state, action_space_size):\n",
    "    \"\"\" Chooses a random action with p_rand_move probability,\n",
    "    otherwise choose the action with highest Q value for\n",
    "    current observation\n",
    "\n",
    "    Args:\n",
    "        q_hat: A Q-value table shaped [num_rows, num_col, num_actions] for \n",
    "            grid environment with num_rows rows and num_col columns and num_actions \n",
    "            number of possible actions\n",
    "        epsilon (float): Probability in [0,1] that the agent selects a random \n",
    "            move instead of selecting greedily from Q value\n",
    "        state: A 2-element array with integer element denoting the row and column\n",
    "            that the agent is in\n",
    "        action_space_size (int): number of possible actions\n",
    "    \n",
    "    Returns:\n",
    "        action (int): A number in the range [0, action_space_size-1]\n",
    "            denoting the action the agent will take\n",
    "    \"\"\"\n",
    "    # TODO: Implement your code here\n",
    "    # Hint: Sample from a uniform distribution and check if the sample is below\n",
    "    # a certain threshold\n",
    "    # ...\n",
    "    q_state = q_hat[state]\n",
    "    \n",
    "    if np.all(q_state == 0) or np.random.uniform(0, 1) < epsilon:\n",
    "        act = np.random.randint(action_space_size)\n",
    "        return act\n",
    "    else:\n",
    "        optim = np.argmax(q_state)\n",
    "        return optim\n",
    "    \n",
    "\n",
    "def softmax_policy(q_hat, beta, state):\n",
    "    \"\"\" Choose action using policy derived from Q, using\n",
    "    softmax of the Q values divided by the temperature.\n",
    "\n",
    "    Args:\n",
    "        q_hat: A Q-value table shaped [num_rows, num_col, num_actions] for \n",
    "            grid environment with num_rows rows and num_col columns\n",
    "        beta (float): Parameter for controlling the stochasticity of the action\n",
    "        obs: A 2-element array with integer element denoting the row and column\n",
    "            that the agent is in\n",
    "\n",
    "    Returns:\n",
    "        action (int): A number in the range [0, action_space_size-1]\n",
    "            denoting the action the agent will take\n",
    "    \"\"\"\n",
    "    # TODO: Implement your code here\n",
    "    # Hint: use the stable_softmax function defined below\n",
    "    # ...\n",
    "    q_state = q_hat[state]\n",
    "    softmax_q_state = stable_softmax(beta * q_state, axis = 0)\n",
    "    return np.random.choice(4, p = softmax_q_state)\n",
    "\n",
    "def beta_exp_schedule(init_beta, iteration, k = 0.1):\n",
    "    beta = init_beta * np.exp(k * iteration)\n",
    "    return beta\n",
    "\n",
    "def stable_softmax(x, axis=2):\n",
    "    \"\"\" Numerically stable softmax:\n",
    "    softmax(x) = e^x /(sum(e^x))\n",
    "               = e^x / (e^max(x) * sum(e^x/e^max(x)))\n",
    "    \n",
    "    Args:\n",
    "        x: An N-dimensional array of floats\n",
    "        axis: The axis for normalizing over.\n",
    "    \n",
    "    Returns:\n",
    "        output: softmax(x) along the specified dimension\n",
    "    \"\"\"\n",
    "    max_x = np.max(x, axis, keepdims = True)\n",
    "    z = np.exp(x - max_x)\n",
    "    output = z / np.sum(z, axis, keepdims = True)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting_utlis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "# from qlearning import *\n",
    "# from maze import *\n",
    "\n",
    "#  UTILITY FUNCTIONS\n",
    "\n",
    "\n",
    "color_cycle = ['#377eb8', '#ff7f00', '#a65628',\n",
    "               '#f781bf','#4daf4a',  '#984ea3',\n",
    "               '#999999', '#e41a1c', '#dede00']\n",
    "\n",
    "def plot_steps_vs_iters(steps_vs_iters, block_size=10):\n",
    "    num_iters = len(steps_vs_iters)\n",
    "    block_size = 10\n",
    "    num_blocks = num_iters // block_size\n",
    "    smooted_data = np.zeros(shape=(num_blocks, 1))\n",
    "    for i in range(num_blocks):\n",
    "        lower = i * block_size\n",
    "        upper = lower + 9\n",
    "        smooted_data[i] = np.mean(steps_vs_iters[lower:upper])\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Steps to goal vs episodes\")\n",
    "    plt.ylabel(\"Steps to goal\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.plot(np.arange(1,num_iters,block_size), smooted_data, color=color_cycle[0])\n",
    "    \n",
    "    return\n",
    "\n",
    "def plot_several_steps_vs_iters(steps_vs_iters_list, label_list, block_size=10):\n",
    "    smooted_data_list = []\n",
    "    for steps_vs_iters in steps_vs_iters_list:\n",
    "        num_iters = len(steps_vs_iters)\n",
    "        block_size = 10\n",
    "        num_blocks = num_iters // block_size\n",
    "        smooted_data = np.zeros(shape=(num_blocks, 1))\n",
    "        for i in range(num_blocks):\n",
    "            lower = i * block_size\n",
    "            upper = lower + 9\n",
    "            smooted_data[i] = np.mean(steps_vs_iters[lower:upper])\n",
    "        smooted_data_list.append(smooted_data)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Steps to goal vs episodes\")\n",
    "    plt.ylabel(\"Steps to goal\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    index = 0\n",
    "    for label, smooted_data in zip(label_list, smooted_data_list):\n",
    "        plt.plot(np.arange(1,num_iters,block_size), smooted_data, label=label, color=color_cycle[index])\n",
    "        index += 1\n",
    "    plt.legend()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "# this function sets color values for \n",
    "# Q table cells depending on expected reward value\n",
    "def get_color(value, min_val, max_val):\n",
    "    \n",
    "    switcher={\n",
    "                0:'gray',\n",
    "                1:'indigo',\n",
    "                2:'darkmagenta',\n",
    "                3:'orchid',\n",
    "                4:'lightpink',\n",
    "             }\n",
    "\n",
    "    step = (max_val-min_val)/5\n",
    "    i = 0\n",
    "    color='lightpink'\n",
    "     \n",
    "    for limit in np.arange(min_val, max_val, step):\n",
    "        if limit <= value < limit+step:\n",
    "            color = switcher.get(i)\n",
    "        i+=1\n",
    "    return color\n",
    "\n",
    "\n",
    "\n",
    "# get first cell out of the start state\n",
    "def get_next_cell(x1,x2,heatmap,policy_table,xlim=9,ylim=9):\n",
    "    up_reward=-10000 \n",
    "    down_reward=-10000 \n",
    "    left_reward=-10000 \n",
    "    right_reward=-10000 \n",
    "\n",
    "    if (x1<ylim):\n",
    "        if (policy_table[x1-1][x2]!=3):\n",
    "            up_reward = heatmap[x1-1][x2]\n",
    "    else: \n",
    "        up_reward = -1000\n",
    "        \n",
    "    if (x1>0):\n",
    "        if (policy_table[x1+1][x2]!=0):\n",
    "            down_reward = heatmap[x1+1][x2]\n",
    "    else: \n",
    "        down_reward = -1000\n",
    "        \n",
    "    if (x2>0):\n",
    "        if (policy_table[x1][x2-1]!=1):\n",
    "            left_reward = heatmap[x1][x2-1] \n",
    "            \n",
    "    else:\n",
    "        left_reward = -1000\n",
    "    \n",
    "    if (x2<xlim):\n",
    "        if (policy_table[x1][x2+1]!=2):\n",
    "            right_reward = heatmap[x1][x2+1] \n",
    "            \n",
    "    else:\n",
    "        right_reward = -1000\n",
    "    \n",
    "    rewards = np.array([up_reward, down_reward, left_reward, right_reward])\n",
    "    idx = np.argmax(rewards)\n",
    "    next_cell = [(x1-1,x2), (x1+1,x2), (x1,x2-1), (x1,x2+1)][idx]\n",
    "    choice = ['up', 'down', 'left', 'right']\n",
    "    #print ('picking ',choice[idx])\n",
    "    return next_cell\n",
    "    \n",
    " \n",
    " \n",
    "\n",
    "# get coordinates of the cells\n",
    "# on the way from the start to goal state \n",
    "def get_path(x1,x2, policy_table):\n",
    "    x_coords = [x1]\n",
    "    y_coords = [x2]\n",
    "    x1_new = x1\n",
    "    x2_new = x2\n",
    "        \n",
    "    i=0\n",
    "    num_steps = 0\n",
    "    total_cells = len(policy_table)*len(policy_table[0])\n",
    "    while (policy_table[x1][x2]!='G') and num_steps < total_cells:\n",
    "        if (policy_table[x1][x2]==1): # right\n",
    "            x2_new=x2+1\n",
    "            #print(i, ' - moving right')\n",
    "            \n",
    "        elif (policy_table[x1][x2]==0):\n",
    "            x1_new=x1-1\n",
    "            #print(i, ' - moving up')\n",
    "            \n",
    "        elif (policy_table[x1][x2]==3):\n",
    "            x1_new=x1+1  \n",
    "            #print(i, ' - moving down')\n",
    "        \n",
    "        elif (policy_table[x1][x2]==2):\n",
    "            x2_new=x2-1 \n",
    "            #print(i, ' - moving left')\n",
    "        \n",
    "        x1 = x1_new\n",
    "        x2 = x2_new\n",
    "        x_coords.append(x1)\n",
    "        y_coords.append(x2)\n",
    "        num_steps += 1\n",
    "    return x_coords, y_coords\n",
    "\n",
    "\n",
    "\n",
    "# plot Q table \n",
    "# optimal path is highlighted and cells colored by their values\n",
    "def plot_table(env, table_data, heatmap, goal_states, start_state, max_val, min_val, x_coords, y_coords):\n",
    "    fig = plt.figure(dpi=80)\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    width = len(table_data[0])\n",
    "    height = len(table_data)\n",
    "    \n",
    "    new_table = []\n",
    "     \n",
    "    for i in range(height):\n",
    "        new_row = []\n",
    "         \n",
    "        for j in range(width):\n",
    "            if env.map[i][j] == 0:\n",
    "                new_row.append('')\n",
    "            else:\n",
    "                digit = table_data[i][j]\n",
    "                if (digit==0):\n",
    "                    new_row.append('\\u2191') # up\n",
    "                elif (digit==1):\n",
    "                    new_row.append('\\u2192') # right\n",
    "                elif (digit==2):\n",
    "                    new_row.append('\\u2190') # left\n",
    "                elif (digit==3):\n",
    "                    new_row.append('\\u2193') # down\n",
    "                elif (digit=='G'):\n",
    "                    new_row.append('G') # goal state\n",
    "                elif (digit=='S'):\n",
    "                    new_row.append('S') # goal state\n",
    "                elif (digit==-1):\n",
    "                    new_row.append('+') # All four directions\n",
    "                else:\n",
    "                    new_row.append('x') # unknown\n",
    "\n",
    "        new_table.append(new_row)\n",
    "\n",
    "    table = ax.table(cellText=new_table, loc='center',cellLoc='center')\n",
    "     \n",
    "    table.scale(1,2)\n",
    "    \n",
    "    for i in range(height):\n",
    "        new_row = []\n",
    "         \n",
    "        for j in range(width):\n",
    "            if new_table[i][j] == '':\n",
    "                table[i, j].set_facecolor('black')\n",
    "            else:\n",
    "                table[i, j].set_facecolor(get_color(heatmap[i][j],min_val,max_val))\n",
    "    \n",
    "    for goal_state in goal_states:\n",
    "        table[(goal_state[0], goal_state[1])].set_facecolor(\"limegreen\")\n",
    "    table[(start_state[0], start_state[1])].set_facecolor(\"yellow\")\n",
    "    ax.axis('off')\n",
    "    table.set_fontsize(16)\n",
    "    \n",
    "    for i in range(len(x_coords)):\n",
    "        table[(x_coords[i], y_coords[i])].get_text().set_color('red')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# this function takes 3D Q table as an input\n",
    "# and outputs optimal trajectory table (policy table)\n",
    "# and corresponding excpected reward values of different cells (heatmap)\n",
    "def get_policy_table(q_hat_3D, start_state, goal_states):\n",
    "    policy_table = []\n",
    "    heatmap = []\n",
    "    \n",
    "    for i in range(q_hat_3D.shape[0]):\n",
    "        row = []\n",
    "        heatmap_row = []\n",
    "        for j in range(q_hat_3D.shape[1]):\n",
    "\n",
    "            heatmap_row.append(np.max(q_hat_3D[i,j,:]))\n",
    "\n",
    "            for goal_state in goal_states:\n",
    "                if (goal_state[0]==i) and (goal_state[1]==j):\n",
    "                    row.append('G')\n",
    "                    \n",
    "            if (start_state[0]==i) and (start_state[1]==j):\n",
    "                row.append('S')\n",
    "            else:\n",
    "                if np.max(q_hat_3D[i,j,:]) == 0:\n",
    "                    row.append(-1) # All zeros\n",
    "                else:\n",
    "                    row.append(np.argmax(q_hat_3D[i,j,:]))\n",
    "        policy_table.append(row)\n",
    "        heatmap.append(heatmap_row)\n",
    "    \n",
    "    return policy_table, heatmap\n",
    "\n",
    "def plot_policy_from_q(q_hat, env):\n",
    "    q_hat_3D = np.reshape(q_hat, (env.m_size, env.m_size, env.num_actions))\n",
    "    max_val = q_hat_3D.max()\n",
    "    min_val = q_hat_3D.min()\n",
    "    start_state = env.get_coords_from_state(env._get_start_state)\n",
    "    goal_states = env._get_goal_state\n",
    "    goal_states = [env.get_coords_from_state(goal_state) for goal_state in goal_states]\n",
    "    policy_table, heatmap = get_policy_table(q_hat_3D, start_state, goal_states)\n",
    "    x,y = get_next_cell(start_state[0],start_state[1],heatmap,policy_table)\n",
    "    x_coords, y_coords = get_path(x,y,policy_table)\n",
    "    plot_table(env, policy_table, heatmap, goal_states, start_state,max_val,min_val, x_coords, y_coords)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## maze.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "ACTION_MEANING = {\n",
    "    0: \"UP\",\n",
    "    1: \"RIGHT\",\n",
    "    2: \"LEFT\",\n",
    "    3: \"DOWN\",\n",
    "}\n",
    "\n",
    "SPACE_MEANING = {\n",
    "    1: \"ROAD\",\n",
    "    0: \"BARRIER\",\n",
    "    -1: \"GOAL\",\n",
    "}\n",
    "\n",
    "\n",
    "class MazeEnv:\n",
    "\n",
    "    def __init__(self, start=[6,3], goals=[[1, 8]]):\n",
    "        \"\"\"Deterministic Maze Environment\"\"\"\n",
    "\n",
    "        self.m_size = 10\n",
    "        self.reward = 10\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.m_size * self.m_size\n",
    "\n",
    "        self.map = np.ones((self.m_size, self.m_size))\n",
    "        self.map[3, 4:9] = 0\n",
    "        self.map[4:8, 4] = 0\n",
    "        self.map[5, 2:4] = 0\n",
    "\n",
    "        for goal in goals:\n",
    "            self.map[goal[0], goal[1]] = -1\n",
    "\n",
    "        self.start = start\n",
    "        self.goals = goals\n",
    "        self.obs = self.start\n",
    "\n",
    "    def step(self, a):\n",
    "        \"\"\" Perform a action on the environment\n",
    "\n",
    "            Args:\n",
    "                a (int): action integer\n",
    "\n",
    "            Returns:\n",
    "                obs (list): observation list\n",
    "                reward (int): reward for such action\n",
    "                done (int): whether the goal is reached\n",
    "        \"\"\"\n",
    "        done, reward = False, 0.0\n",
    "        next_obs = copy.copy(self.obs)\n",
    "\n",
    "        if a == 0:\n",
    "            next_obs[0] = next_obs[0] - 1\n",
    "        elif a == 1:\n",
    "            next_obs[1] = next_obs[1] + 1\n",
    "        elif a == 2:\n",
    "            next_obs[1] = next_obs[1] - 1\n",
    "        elif a == 3:\n",
    "            next_obs[0] = next_obs[0] + 1\n",
    "        else:\n",
    "            raise Exception(\"Action is Not Valid\")\n",
    "\n",
    "        if self.is_valid_obs(next_obs):\n",
    "            self.obs = next_obs\n",
    "\n",
    "        if self.map[self.obs[0], self.obs[1]] == -1:\n",
    "            reward = self.reward\n",
    "            done = True\n",
    "        \n",
    "        state = self.get_state_from_coords(self.obs[0], self.obs[1])\n",
    "\n",
    "        return state, reward, done\n",
    "\n",
    "    def is_valid_obs(self, obs):\n",
    "        \"\"\" Check whether the observation is valid\n",
    "\n",
    "            Args:\n",
    "                obs (list): observation [x, y]\n",
    "\n",
    "            Returns:\n",
    "                is_valid (bool)\n",
    "        \"\"\"\n",
    "\n",
    "        if obs[0] >= self.m_size or obs[0] < 0:\n",
    "            return False\n",
    "\n",
    "        if obs[1] >= self.m_size or obs[1] < 0:\n",
    "            return False\n",
    "\n",
    "        if self.map[obs[0], obs[1]] == 0:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def _get_obs(self):\n",
    "        \"\"\" Get current observation\n",
    "        \"\"\"\n",
    "        return self.obs\n",
    "    \n",
    "    @property\n",
    "    def _get_state(self):\n",
    "        \"\"\" Get current observation\n",
    "        \"\"\"\n",
    "        return self.get_state_from_coords(self.obs[0], self.obs[1])\n",
    "    \n",
    "    @property\n",
    "    def _get_start_state(self):\n",
    "        \"\"\" Get the start state\n",
    "        \"\"\"\n",
    "        return self.get_state_from_coords(self.start[0], self.start[1])\n",
    "    \n",
    "    @property\n",
    "    def _get_goal_state(self):\n",
    "        \"\"\" Get the start state\n",
    "        \"\"\"\n",
    "        goals = []\n",
    "        for goal in self.goals:\n",
    "            goals.append(self.get_state_from_coords(goal[0], goal[1]))\n",
    "        return goals\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Reset the observation into starting point\n",
    "        \"\"\"\n",
    "        self.obs = self.start\n",
    "        state = self.get_state_from_coords(self.obs[0], self.obs[1])\n",
    "        return state\n",
    "    \n",
    "    def get_state_from_coords(self, row, col):\n",
    "        state = row * self.m_size + col\n",
    "        return state\n",
    "    \n",
    "    def get_coords_from_state(self, state):\n",
    "        row = math.floor(state/self.m_size)\n",
    "        col = state % self.m_size \n",
    "        return row, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Q Learning experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Run your algorithm several times on the given environment. Use the following hyperparameters:\n",
    "1. Number of episodes = 200\n",
    "2. Alpha ($\\alpha$) learning rate = 1.0\n",
    "2. Maximum number of steps per episode = 100. An episode ends when the agent reaches a goal state, or uses the maximum number of steps per episode\n",
    "3. Gamma ($\\gamma$) discount factor = 0.9\n",
    "4. Epsilon ($\\epsilon$) for $\\epsilon$-greedy = 0.1 (10% of the time). Note that we should \"break-ties\" when the Q-values are zero for all the actions (happens initially) by essentially choosing uniformly from the action. So now you have two conditions to act randomly: for epsilon amount of the time, or if the Q values are all zero. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill this in\n",
    "num_iters = 200\n",
    "alpha = 1.0\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "max_steps = 100\n",
    "use_softmax_policy = False\n",
    "\n",
    "# TODO: Instantiate the MazeEnv environment with default arguments\n",
    "env = MazeEnv()\n",
    "\n",
    "# TODO: Run Q-learning:\n",
    "q_hat, steps_vs_iters = qlearn(env, num_iters, alpha, gamma, epsilon, max_steps, use_softmax_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the steps to goal vs training iterations (episodes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the steps vs iterations\n",
    "plot_steps_vs_iters(steps_vs_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the learned greedy policy from the Q values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: plot the policy from the Q value\n",
    "plot_policy_from_q(q_hat, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Run your algorithm by passing in a list of 2 goal locations: (1,8) and (5,6). Note: we are using 0-indexing, where (0,0) is top left corner. Report on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill this in (same as before)\n",
    "num_iters = 200\n",
    "alpha = 1.0\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "max_steps = 100\n",
    "use_softmax_policy = False\n",
    "\n",
    "# TODO: Set the goal\n",
    "goal_locs = [[1, 8], [5, 6]]\n",
    "env = MazeEnv(start = [6, 3], goals = goal_locs)\n",
    "\n",
    "# TODO: Run Q-learning:\n",
    "q_hat, steps_vs_iters = qlearn(env, num_iters, alpha, gamma, epsilon, max_steps, use_softmax_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the steps to goal vs training iterations (episodes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the steps vs iterations\n",
    "plot_steps_vs_iters(steps_vs_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the steps to goal vs training iterations (episodes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the policy from the Q values\n",
    "plot_policy_from_q(q_hat, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment with the exploration strategy, in the original environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Try different $\\epsilon$ values in $\\epsilon$-greedy exploration: We asked you to use a rate of $\\epsilon$=10%, but try also 50% and 1%. Graph the results (for 3 epsilon values) and discuss the costs and benefits of higher and lower exploration rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill this in (same as before)\n",
    "num_iters = 200\n",
    "alpha = 1.0\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "max_steps = 100\n",
    "use_softmax_policy = False\n",
    "\n",
    "# TODO: set the epsilon lists in increasing order:\n",
    "epsilon_list = [0.01, 0.1, 0.5]\n",
    "\n",
    "env = MazeEnv()\n",
    "\n",
    "steps_vs_iters_list = []\n",
    "for epsilon in epsilon_list:\n",
    "    q_hat, steps_vs_iters = qlearn(env, num_iters, alpha, gamma, epsilon, max_steps, use_softmax_policy)\n",
    "    steps_vs_iters_list.append(steps_vs_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the results\n",
    "label_list = [\"epsilon={}\".format(eps) for eps in epsilon_list]\n",
    "plot_several_steps_vs_iters(steps_vs_iters_list, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Try exploring with policy derived from **softmax of Q-values** described in the Q learning lecture. Use the values of $\\beta \\in \\{1, 3, 6\\}$ for your experiment, keeping $\\beta$ fixed throughout the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill this in for Static Beta with softmax of Q-values\n",
    "num_iters = 200\n",
    "alpha = 1.0\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "max_steps = 100\n",
    "\n",
    "# TODO: Set the beta\n",
    "beta_list = [1, 3, 6]\n",
    "use_softmax_policy = True\n",
    "k_exp_schedule = 0\n",
    "\n",
    "env = MazeEnv()\n",
    "steps_vs_iters_list = []\n",
    "for beta in beta_list:\n",
    "    q_hat, steps_vs_iters = qlearn(env, num_iters, alpha, gamma, epsilon, max_steps, use_softmax_policy, beta, k_exp_schedule)\n",
    "    steps_vs_iters_list.append(steps_vs_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"beta={}\".format(beta) for beta in beta_list]\n",
    "# TODO: \n",
    "plot_several_steps_vs_iters(steps_vs_iters_list, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Instead of fixing the $\\beta = \\beta_0$ to the initial value, we will increase the value of $\\beta$ as the number of episodes $t$ increase:\n",
    "\n",
    "$$\\beta(t) = \\beta_0 e^{kt}$$\n",
    "\n",
    "That is, the $\\beta$ value is fixed for a particular episode.\n",
    "Run the training again for different values of $k \\in \\{0.05, 0.1, 0.25, 0.5\\}$, keeping $\\beta_0 = 1.0$. Compare the results obtained with this approach to those obtained with a static $\\beta$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill this in for Dynamic Beta\n",
    "num_iters = 200\n",
    "alpha = 1.0\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "max_steps = 100\n",
    "\n",
    "# TODO: Set the beta\n",
    "beta = 1.0\n",
    "use_softmax_policy = True\n",
    "k_exp_schedule_list = [0.05, 0.1, 0.25, 0.5]\n",
    "env = MazeEnv()\n",
    "\n",
    "steps_vs_iters_list = []\n",
    "for k_exp_schedule in k_exp_schedule_list:\n",
    "    q_hat, steps_vs_iters = qlearn(env, num_iters, alpha, gamma, epsilon, max_steps, use_softmax_policy, beta, k_exp_schedule)\n",
    "    steps_vs_iters_list.append(steps_vs_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the steps vs iterations\n",
    "label_list = [\"k={}\".format(k_exp_schedule) for k_exp_schedule in k_exp_schedule_list]\n",
    "plot_several_steps_vs_iters(steps_vs_iters_list, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stochastic Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Make  the  environment  stochastic  (uncertain),  such  that  the  agent  only  has  a  95% chance  of  moving  in  the  chosen  direction,  and  has  a  5%  chance  of  moving  in  some random direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement ProbabilisticMazeEnv in maze.py\n",
    "class ProbabilisticMazeEnv(MazeEnv):\n",
    "    \"\"\" (Q2.3) Hints: you can refer the implementation in MazeEnv \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, goals=[[2, 8]], p_random=0.05):\n",
    "        \"\"\" Probabilistic Maze Environment \n",
    "\n",
    "            Args:\n",
    "                goals (list): list of goals coordinates\n",
    "                p_random (float): random action rate\n",
    "        \"\"\"\n",
    "        super(ProbabilisticMazeEnv, self).__init__(goals=goals)\n",
    "        self.p_random = p_random\n",
    "\n",
    "\n",
    "    def step(self, a):\n",
    "        \"\"\" Perform a action on the environment\n",
    "\n",
    "            Args:\n",
    "                a (int): action integer\n",
    "\n",
    "            Returns:\n",
    "                obs (list): observation list\n",
    "                reward (int): reward for such action\n",
    "                done (int): whether the goal is reached\n",
    "        \"\"\"\n",
    "        done, reward = False, 0.0\n",
    "        next_obs = copy.copy(self.obs)\n",
    "\n",
    "        if np.random.uniform(0, 1) < self.p_random:\n",
    "            a = np.random.randint(self.num_actions)\n",
    "\n",
    "        if a == 0:\n",
    "            next_obs[0] = next_obs[0] - 1\n",
    "        elif a == 1:\n",
    "            next_obs[1] = next_obs[1] + 1\n",
    "        elif a == 2:\n",
    "            next_obs[1] = next_obs[1] - 1\n",
    "        elif a == 3:\n",
    "            next_obs[0] = next_obs[0] + 1\n",
    "        else:\n",
    "            raise Exception(\"Action is Not Valid\")\n",
    "\n",
    "        if self.is_valid_obs(next_obs):\n",
    "            self.obs = next_obs\n",
    "\n",
    "        if self.map[self.obs[0], self.obs[1]] == -1:\n",
    "            reward = self.reward\n",
    "            done = True\n",
    "            \n",
    "        state = self.get_state_from_coords(self.obs[0], self.obs[1])\n",
    "\n",
    "        return state, reward, done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 200\n",
    "alpha = 1\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "max_steps = 100\n",
    "use_softmax_policy = False\n",
    "\n",
    "# Set the environment probability of random\n",
    "env_p_rand_list = [0.05]\n",
    "\n",
    "steps_vs_iters_list = []\n",
    "for env_p_rand in env_p_rand_list:\n",
    "    # Instantiate with ProbabilisticMazeEnv\n",
    "    env = ProbabilisticMazeEnv(p_random = env_p_rand)\n",
    "\n",
    "    # Note: We will repeat for several runs of the algorithm to make the result less noisy\n",
    "    avg_steps_vs_iters = np.zeros(num_iters)\n",
    "    for i in range(10):\n",
    "        q_hat, steps_vs_iters = qlearn(env, num_iters, alpha, gamma, epsilon, max_steps, use_softmax_policy)\n",
    "        avg_steps_vs_iters += steps_vs_iters\n",
    "    avg_steps_vs_iters /= 10\n",
    "    steps_vs_iters_list.append(avg_steps_vs_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"env_random={}\".format(env_p_rand) for env_p_rand in env_p_rand_list]\n",
    "plot_several_steps_vs_iters(steps_vs_iters_list, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Change the learning rule to handle the non-determinism, and experiment with different probability of environment performing random action $p_{rand} \\in \\{0.05, 0.1, 0.25, 0.5\\}$ in this new rule. How does performance vary as the environment becomes more stochastic?\n",
    "\n",
    "Use the same parameters as in first part, except change the alpha ($\\alpha$) value to be **less than 1**, e.g. 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the same parameters as in the first part, except change alpha\n",
    "num_iters = 200\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "max_steps = 100\n",
    "use_softmax_policy = False\n",
    "\n",
    "# Set the environment probability of random\n",
    "env_p_rand_list = [0.05, 0.1, 0.25, 0.5]\n",
    "\n",
    "steps_vs_iters_list = []\n",
    "for env_p_rand in env_p_rand_list:\n",
    "    # Instantiate with ProbabilisticMazeEnv\n",
    "    env = ProbabilisticMazeEnv(p_random = env_p_rand)\n",
    "\n",
    "    # Note: We will repeat for several runs of the algorithm to make the result less noisy\n",
    "    avg_steps_vs_iters = np.zeros(num_iters)\n",
    "    for i in range(10):\n",
    "        q_hat, steps_vs_iters = qlearn(env, num_iters, alpha, gamma, epsilon, max_steps, use_softmax_policy)\n",
    "        avg_steps_vs_iters += steps_vs_iters\n",
    "    avg_steps_vs_iters /= 10\n",
    "    steps_vs_iters_list.append(avg_steps_vs_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"env_random={}\".format(env_p_rand) for env_p_rand in env_p_rand_list]\n",
    "plot_several_steps_vs_iters(steps_vs_iters_list, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Did you complete the course evaluation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer: yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
